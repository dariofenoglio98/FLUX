{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5 (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316792\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.817093\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.199695\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.175400\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.296527\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.156364\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.231877\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.090156\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.182235\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.116326\n",
      "\n",
      "Test set: Average loss: 0.0909, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.138631\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.121906\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.031991\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.108236\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.121946\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.011026\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.043982\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.019277\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.066178\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.055668\n",
      "\n",
      "Test set: Average loss: 0.0542, Accuracy: 9828/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.054563\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.065500\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.069394\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.035769\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.051561\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.040762\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.082672\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.011585\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.069181\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.005766\n",
      "\n",
      "Test set: Average loss: 0.0549, Accuracy: 9820/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.074824\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.007413\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.005517\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.003330\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.057359\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.019024\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.034037\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.011163\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.046258\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.024150\n",
      "\n",
      "Test set: Average loss: 0.0355, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.039560\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.062972\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.104506\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.004616\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.055692\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.017744\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.078841\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.092283\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.039554\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.054604\n",
      "\n",
      "Test set: Average loss: 0.0363, Accuracy: 9877/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.007433\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.006041\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.044478\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.022868\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.018344\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.022747\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.022776\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.005340\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.002316\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.027337\n",
      "\n",
      "Test set: Average loss: 0.0363, Accuracy: 9889/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.155126\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.003731\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.018581\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.010054\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.003724\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.029007\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.028931\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.006117\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.011405\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.007146\n",
      "\n",
      "Test set: Average loss: 0.0306, Accuracy: 9899/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.008685\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.026892\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.018573\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.002858\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.006222\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.004431\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.064298\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.002852\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.006127\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003183\n",
      "\n",
      "Test set: Average loss: 0.0306, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.040020\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.012606\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.026977\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.009214\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001929\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.035390\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.000657\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.052453\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.003098\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.001455\n",
      "\n",
      "Test set: Average loss: 0.0269, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.008246\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.005364\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.002709\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.001646\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000699\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000325\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.001447\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.007127\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.043976\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.000558\n",
      "\n",
      "Test set: Average loss: 0.0322, Accuracy: 9898/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 6, kernel_size=5, stride=1, padding=2)  # Convolutional layer with 6 feature maps of size 5x5\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)  # Subsampling layer with 6 feature maps of size 2x2\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)  # Convolutional layer with 16 feature maps of size 5x5\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)  # Subsampling layer with 16 feature maps of size 2x2\n",
    "        \n",
    "        # These layers depend on the input size\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # Fully connected layer, output size 120\n",
    "        self.fc2 = nn.Linear(120, 84)  # Fully connected layer, output size 84\n",
    "        self.fc3 = nn.Linear(84, num_classes)  # Fully connected layer, output size num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # Apply ReLU after conv1\n",
    "        x = self.pool1(x)  # Apply subsampling pool1\n",
    "        x = F.relu(self.conv2(x))  # Apply ReLU after conv2\n",
    "        x = self.pool2(x)  # Apply subsampling pool2\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU after fc1\n",
    "        x = F.relu(self.fc2(x))  # Apply ReLU after fc2\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Define a function to train the model\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# Define a function to test the model\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "    \n",
    "# define device\n",
    "def check_gpu(manual_seed=True, print_info=True):\n",
    "    if manual_seed:\n",
    "        torch.manual_seed(0)\n",
    "    if torch.cuda.is_available():\n",
    "        if print_info:\n",
    "            print(\"CUDA is available\")\n",
    "        device = 'cuda'\n",
    "        torch.cuda.manual_seed_all(0) \n",
    "    elif torch.backends.mps.is_available():\n",
    "        if print_info:\n",
    "            print(\"MPS is available\")\n",
    "        device = torch.device(\"mps\")\n",
    "        torch.mps.manual_seed(0)\n",
    "    else:\n",
    "        if print_info:\n",
    "            print(\"CUDA is not available\")\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "# Main function to run the training and testing\n",
    "def main():\n",
    "    # Training settings\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    epochs = 10\n",
    "    lr = 0.01\n",
    "    momentum = 0.9\n",
    "    no_cuda = False\n",
    "    seed = 1\n",
    "\n",
    "    device = check_gpu(manual_seed=True, print_info=True)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    model = LeNet5(in_channels=1, num_classes=10).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-9 (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.393180\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.125153\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.061068\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.140412\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.020400\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.041769\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.004259\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.029730\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.007122\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.084373\n",
      "\n",
      "Test set: Average loss: 0.0489, Accuracy: 9847/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.074853\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.016679\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.052707\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.055934\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.004753\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.023614\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.115123\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.003418\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.018911\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.004706\n",
      "\n",
      "Test set: Average loss: 0.0277, Accuracy: 9906/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.020622\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.004132\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.000469\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.003857\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.019780\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.016303\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.004763\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.053840\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.085678\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.030130\n",
      "\n",
      "Test set: Average loss: 0.0262, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.001537\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.013710\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.006243\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.019015\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.004558\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.013583\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.002078\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.000520\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.009588\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.007469\n",
      "\n",
      "Test set: Average loss: 0.0202, Accuracy: 9942/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.001560\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.000405\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.000278\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.001224\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.038621\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.004806\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.000261\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.000063\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.001285\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.014120\n",
      "\n",
      "Test set: Average loss: 0.0170, Accuracy: 9952/10000 (100%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.001588\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.000224\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.000305\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.000154\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.000201\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.004838\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.000901\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.000097\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.000648\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.000989\n",
      "\n",
      "Test set: Average loss: 0.0159, Accuracy: 9956/10000 (100%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.001478\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.000111\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.001215\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001083\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.000181\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.000939\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.000340\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.020601\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.000151\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.007565\n",
      "\n",
      "Test set: Average loss: 0.0176, Accuracy: 9946/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.000060\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.000072\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.000316\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.077998\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.001265\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.003571\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.000080\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.000818\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.000254\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.004676\n",
      "\n",
      "Test set: Average loss: 0.0185, Accuracy: 9943/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.000207\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.000062\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000029\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000058\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.000203\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.000093\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.000224\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.000728\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.000062\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.001300\n",
      "\n",
      "Test set: Average loss: 0.0194, Accuracy: 9941/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000113\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.000358\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.002204\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000817\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000080\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000959\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.000054\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.000070\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.000045\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.002585\n",
      "\n",
      "Test set: Average loss: 0.0287, Accuracy: 9913/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the conv_bn_relu_pool function\n",
    "def conv_bn_relu_pool(in_channels, out_channels, pool=False):\n",
    "    layers = [\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    ]\n",
    "    if pool:\n",
    "        layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.prep = conv_bn_relu_pool(in_channels, 64)\n",
    "        self.layer1_head = conv_bn_relu_pool(64, 128, pool=True)\n",
    "        self.layer1_residual = nn.Sequential(conv_bn_relu_pool(128, 128), conv_bn_relu_pool(128, 128))\n",
    "        self.layer2 = conv_bn_relu_pool(128, 256, pool=True)\n",
    "        self.layer3_head = conv_bn_relu_pool(256, 512, pool=True)\n",
    "        self.layer3_residual = nn.Sequential(conv_bn_relu_pool(512, 512), conv_bn_relu_pool(512, 512))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Changed to adaptive average pooling:         self.MaxPool2d = nn.Sequential(nn.MaxPool2d(4))\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prep(x)\n",
    "        x = self.layer1_head(x)\n",
    "        x = self.layer1_residual(x) + x\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3_head(x)\n",
    "        x = self.layer3_residual(x) + x\n",
    "        x = self.avgpool(x)  # Changed to adaptive average pooling\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Define a function to train the model\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# Define a function to test the model\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "\n",
    "# Main function to run the training and testing\n",
    "def main():\n",
    "    # Training settings\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    epochs = 10\n",
    "    lr = 0.01\n",
    "    momentum = 0.9\n",
    "    no_cuda = False\n",
    "    seed = 1\n",
    "\n",
    "    device = check_gpu(manual_seed=True, print_info=True)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    model = ResNet9(in_channels=1, num_classes=10).to(device)  # Set in_channels to 1 for MNIST\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet-5 model has 61,706 parameters\n",
      "ResNet-9 model has 6,573,705 parameters\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "model_lenet = LeNet5()\n",
    "print(f'LeNet-5 model has {sum(p.numel() for p in model_lenet.parameters()):,} parameters')\n",
    "model_resnet9 = ResNet9(in_channels=1, num_classes=9)\n",
    "print(f'ResNet-9 model has {sum(p.numel() for p in model_resnet9.parameters()):,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainings on ours dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 18718859.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 3326388.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 20956104.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 12055989.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:19<00:00, 8610973.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:19<00:00, 8677287.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import non_iiddata_generator_no_drifting as noniidgen\n",
    "\n",
    "def data_creation(dataset_name):\n",
    "    # Load the dataset\n",
    "    # Options: \"MNIST\", \"FMNIST\", \"CIFAR10\", \"CIFAR100\", etc.\n",
    "    train_images, train_labels, test_images, test_labels = noniidgen.load_full_datasets(dataset_name)\n",
    "\n",
    "    # Define parameters for split_feature_skew\n",
    "    client_number = 10\n",
    "    set_rotation = True\n",
    "    rotations = 4\n",
    "    scaling_rotation_low = 0.1\n",
    "    scaling_rotation_high = 0.2\n",
    "    set_color = True\n",
    "    colors = 3\n",
    "    scaling_color_low = 0.1\n",
    "    scaling_color_high = 0.2\n",
    "    random_order = True\n",
    "\n",
    "    # Run split_feature_skew\n",
    "    clients_data = noniidgen.split_feature_skew(\n",
    "        train_features = train_images,\n",
    "        train_labels = train_labels,\n",
    "        test_features = test_images,\n",
    "        test_labels = test_labels,\n",
    "        client_number = client_number,\n",
    "        set_rotation = set_rotation,\n",
    "        rotations = rotations,\n",
    "        scaling_rotation_low = scaling_rotation_low,\n",
    "        scaling_rotation_high = scaling_rotation_high,\n",
    "        set_color = set_color,\n",
    "        colors = colors,\n",
    "        scaling_color_low = scaling_color_low,\n",
    "        scaling_color_high = scaling_color_high,\n",
    "        random_order = random_order\n",
    "    )\n",
    "    return clients_data\n",
    "\n",
    "data_MNIST = data_creation(\"MNIST\")\n",
    "data_FMNIST = data_creation(\"FMNIST\")\n",
    "data_CIFAR10 = data_creation(\"CIFAR10\")\n",
    "data_CIFAR100 = data_creation(\"CIFAR100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.492195\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.239441\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.164107\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.944633\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.713479\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.004442\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.913772\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.762644\n",
      "\n",
      "Test set: Average loss: 1.7305, Accuracy: 3724/10000 (37%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.895573\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.940766\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.958923\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.622462\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.713544\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.967214\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.407248\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.528836\n",
      "\n",
      "Test set: Average loss: 1.6656, Accuracy: 4110/10000 (41%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.456856\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.339177\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.494506\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.454145\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.425265\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.579947\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.543551\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.415182\n",
      "\n",
      "Test set: Average loss: 1.3105, Accuracy: 5354/10000 (54%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.602923\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.366930\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.235944\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.298732\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.094521\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.063001\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.243723\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.839254\n",
      "\n",
      "Test set: Average loss: 1.3724, Accuracy: 5174/10000 (52%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.069730\n",
      "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.102493\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.870357\n",
      "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.016649\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.056112\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 0.993669\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.031552\n",
      "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.192953\n",
      "\n",
      "Test set: Average loss: 1.1694, Accuracy: 5931/10000 (59%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.104963\n",
      "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 1.062804\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.888746\n",
      "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 0.868818\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.778530\n",
      "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 0.829901\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 1.244481\n",
      "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 1.305514\n",
      "\n",
      "Test set: Average loss: 1.1246, Accuracy: 6184/10000 (62%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.916303\n",
      "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.939575\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.785785\n",
      "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 0.640003\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.022498\n",
      "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 0.894803\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.841459\n",
      "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 0.588679\n",
      "\n",
      "Test set: Average loss: 1.0482, Accuracy: 6402/10000 (64%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.594068\n",
      "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.634710\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.662204\n",
      "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 0.488437\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.581229\n",
      "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.622351\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.726629\n",
      "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.768913\n",
      "\n",
      "Test set: Average loss: 1.0358, Accuracy: 6590/10000 (66%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.554226\n",
      "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 0.646536\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.587347\n",
      "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.523803\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.870944\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 0.717994\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.651073\n",
      "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.620309\n",
      "\n",
      "Test set: Average loss: 1.1104, Accuracy: 6542/10000 (65%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.489968\n",
      "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.509784\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.375537\n",
      "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 0.410504\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.465466\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 0.478827\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.686940\n",
      "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.423189\n",
      "\n",
      "Test set: Average loss: 1.1751, Accuracy: 6542/10000 (65%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import transforms\n",
    "from math import prod\n",
    "\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "def merge_data(data):\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    for client_data in data:\n",
    "        train_features.append(client_data['train_features'])\n",
    "        train_labels.append(client_data['train_labels'])\n",
    "        test_features.append(client_data['test_features'])\n",
    "        test_labels.append(client_data['test_labels'])\n",
    "\n",
    "    # Concatenate all the data\n",
    "    train_features = torch.cat(train_features, dim=0)\n",
    "    train_labels = torch.cat(train_labels, dim=0)\n",
    "    test_features = torch.cat(test_features, dim=0)\n",
    "    test_labels = torch.cat(test_labels, dim=0)\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels\n",
    "    \n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10, input_size=(28, 28)):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 6, kernel_size=5, stride=1, padding=2)  # Convolutional layer with 6 feature maps of size 5x5\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)  # Subsampling layer with 6 feature maps of size 2x2\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)  # Convolutional layer with 16 feature maps of size 5x5\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)  # Subsampling layer with 16 feature maps of size 2x2\n",
    "        \n",
    "        # Calculate the size of the features after convolutional layers\n",
    "        dummy_input = torch.zeros(1, in_channels, *input_size)\n",
    "        dummy_output = self.pool2(self.conv2(self.pool1(self.conv1(dummy_input))))\n",
    "        self.feature_size = prod(dummy_output.size()[1:])\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.feature_size, 120)  # Fully connected layer, output size 120\n",
    "        self.fc2 = nn.Linear(120, 84)  # Fully connected layer, output size 84\n",
    "        self.fc3 = nn.Linear(84, num_classes)  # Fully connected layer, output size num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # Apply ReLU after conv1\n",
    "        x = self.pool1(x)  # Apply subsampling pool1\n",
    "        x = F.relu(self.conv2(x))  # Apply ReLU after conv2\n",
    "        x = self.pool2(x)  # Apply subsampling pool2\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU after fc1\n",
    "        x = F.relu(self.fc2(x))  # Apply ReLU after fc2\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x\n",
    "\n",
    "# Define a function to train the model\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# Define a function to test the model\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "    \n",
    "# define device\n",
    "def check_gpu(manual_seed=True, print_info=True):\n",
    "    if manual_seed:\n",
    "        torch.manual_seed(0)\n",
    "    if torch.cuda.is_available():\n",
    "        if print_info:\n",
    "            print(\"CUDA is available\")\n",
    "        device = 'cuda'\n",
    "        torch.cuda.manual_seed_all(0) \n",
    "    elif torch.backends.mps.is_available():\n",
    "        if print_info:\n",
    "            print(\"MPS is available\")\n",
    "        device = torch.device(\"mps\")\n",
    "        torch.mps.manual_seed(0)\n",
    "    else:\n",
    "        if print_info:\n",
    "            print(\"CUDA is not available\")\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "# Main function to run the training and testing\n",
    "def main():\n",
    "    # Training settings\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    epochs = 10\n",
    "    lr = 0.01\n",
    "    momentum = 0.9\n",
    "    no_cuda = False\n",
    "    seed = 1\n",
    "\n",
    "    device = check_gpu(manual_seed=True, print_info=True)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # merge the data\n",
    "    train_features, train_labels, test_features, test_labels = merge_data(data_CIFAR10)\n",
    "\n",
    "    # Define any necessary transforms\n",
    "    transform = None\n",
    "\n",
    "    # Create the datasets\n",
    "    train_dataset = CombinedDataset(train_features, train_labels, transform=transform)\n",
    "    test_dataset = CombinedDataset(test_features, test_labels, transform=transform)\n",
    "\n",
    "    # Define batch sizes\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "\n",
    "    # Create the data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    # model = LeNet5(in_channels=3, num_classes=10, input_size=(32,32)).to(device)\n",
    "    model = ResNet9(in_channels=3, num_classes=10).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with our models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.311269\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.299786\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.297932\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.296077\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.280352\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.140450\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 2.250665\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 2.247782\n",
      "\n",
      "Test set: Average loss: 2.1686, Accuracy: 2076/10000 (21%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.215068\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 2.160078\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 2.154256\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 2.040477\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 2.118145\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 2.168162\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 2.121017\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 2.161525\n",
      "\n",
      "Test set: Average loss: 2.0218, Accuracy: 2659/10000 (27%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 2.162241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m         simple_test(model, device, test_loader)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 72\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 72\u001b[0m     \u001b[43msimple_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     simple_test(model, device, test_loader)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-USI/PC/Desktop/USI_Locale/cfl_maggio/models.py:135\u001b[0m, in \u001b[0;36msimple_train\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m    133\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    134\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[0;32m--> 135\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    137\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/CF_FL/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "import non_iiddata_generator_no_drifting as noniidgen\n",
    "from non_iiddata_generator_no_drifting import merge_data\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    model_name = \"LeNet5\"   # Options: \"LeNet5\", \"ResNet9\"\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    epochs = 10\n",
    "    lr = 0.01\n",
    "    momentum = 0.9\n",
    "    seed = 1\n",
    "    transform = None\n",
    "    # dataset settings\n",
    "    dataset_name = \"CIFAR10\"\n",
    "    client_number = 10\n",
    "    set_rotation = True\n",
    "    rotations = 4\n",
    "    scaling_rotation_low = 0.1\n",
    "    scaling_rotation_high = 0.2\n",
    "    set_color = True\n",
    "    colors = 3\n",
    "    scaling_color_low = 0.1\n",
    "    scaling_color_high = 0.2\n",
    "    random_order = True\n",
    "\n",
    "    device = check_gpu(manual_seed=True, print_info=True)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # load data \n",
    "    train_images, train_labels, test_images, test_labels = noniidgen.load_full_datasets(dataset_name)\n",
    "\n",
    "    # create data: split_feature_skew\n",
    "    clients_data = noniidgen.split_feature_skew(\n",
    "        train_features = train_images,\n",
    "        train_labels = train_labels,\n",
    "        test_features = test_images,\n",
    "        test_labels = test_labels,\n",
    "        client_number = client_number,\n",
    "        set_rotation = set_rotation,\n",
    "        rotations = rotations,\n",
    "        scaling_rotation_low = scaling_rotation_low,\n",
    "        scaling_rotation_high = scaling_rotation_high,\n",
    "        set_color = set_color,\n",
    "        colors = colors,\n",
    "        scaling_color_low = scaling_color_low,\n",
    "        scaling_color_high = scaling_color_high,\n",
    "        random_order = random_order\n",
    "    )\n",
    "\n",
    "    # merge the data (for Centralized Learning Simulation)\n",
    "    train_features, train_labels, test_features, test_labels = merge_data(clients_data)\n",
    "\n",
    "    # Create the datasets\n",
    "    train_dataset = CombinedDataset(train_features, train_labels, transform=transform)\n",
    "    test_dataset = CombinedDataset(test_features, test_labels, transform=transform)\n",
    "\n",
    "    # Create the data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "    # model = LeNet5(in_channels=3, num_classes=10, input_size=(32,32)).to(device)\n",
    "    model = models[model_name](in_channels=3, num_classes=10, input_size=(32,32)).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        simple_train(model, device, train_loader, optimizer, epoch)\n",
    "        simple_test(model, device, test_loader)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet-5 model has 83,126 parameters\n",
      "ResNet-9 model has 6,590,730 parameters\n",
      "LeNet-5 model has 62,006 parameters\n",
      "ResNet-9 model has 6,575,370 parameters\n"
     ]
    }
   ],
   "source": [
    "# print the number of parameters\n",
    "model_lenet = LeNet5(in_channels=3, num_classes=10, input_size=(32,32))\n",
    "print(f'LeNet-5 model has {sum(p.numel() for p in model_lenet.parameters()):,} parameters')\n",
    "model_resnet9 = ResNet9(in_channels=3, num_classes=10, input_size=(32,32))\n",
    "print(f'ResNet-9 model has {sum(p.numel() for p in model_resnet9.parameters()):,} parameters')\n",
    "\n",
    "model_lenet = LeNet5(in_channels=3, num_classes=10, input_size=(28,28))\n",
    "print(f'LeNet-5 model has {sum(p.numel() for p in model_lenet.parameters()):,} parameters')\n",
    "model_resnet9 = ResNet9(in_channels=3, num_classes=10, input_size=(28,28))\n",
    "print(f'ResNet-9 model has {sum(p.numel() for p in model_resnet9.parameters()):,} parameters')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "Train Epoch: 0 [0/5000 (0%)]\tLoss: 2.305545\n",
      "Train Epoch: 0 [640/5000 (13%)]\tLoss: 2.302148\n",
      "Train Epoch: 0 [1280/5000 (25%)]\tLoss: 2.309596\n",
      "Train Epoch: 0 [1920/5000 (38%)]\tLoss: 2.300848\n",
      "Train Epoch: 0 [2560/5000 (51%)]\tLoss: 2.302197\n",
      "Train Epoch: 0 [3200/5000 (63%)]\tLoss: 2.310703\n",
      "Train Epoch: 0 [3840/5000 (76%)]\tLoss: 2.304026\n",
      "Train Epoch: 0 [4480/5000 (89%)]\tLoss: 2.303044\n",
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 2.303247\n",
      "Train Epoch: 1 [640/5000 (13%)]\tLoss: 2.304303\n",
      "Train Epoch: 1 [1280/5000 (25%)]\tLoss: 2.296026\n",
      "Train Epoch: 1 [1920/5000 (38%)]\tLoss: 2.301974\n",
      "Train Epoch: 1 [2560/5000 (51%)]\tLoss: 2.303057\n",
      "Train Epoch: 1 [3200/5000 (63%)]\tLoss: 2.295039\n",
      "Train Epoch: 1 [3840/5000 (76%)]\tLoss: 2.297525\n",
      "Train Epoch: 1 [4480/5000 (89%)]\tLoss: 2.304764\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import models    \n",
    "import config as cfg\n",
    "import numpy as np\n",
    "import utils\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# check gpu and set manual seed\n",
    "device = utils.check_gpu(manual_seed=True)\n",
    "\n",
    "# model and history folder\n",
    "model = models.models[cfg.model_name](in_channels=3, num_classes=cfg.n_classes, input_size=cfg.input_size).to(device)\n",
    "# train_fn = utils.trainings[args.model]\n",
    "# evaluate_fn = utils.evaluations[args.model]\n",
    "# plot_fn = utils.plot_functions[args.model]\n",
    "# config = utils.config_tests[args.dataset][args.model]\n",
    "\n",
    "# check if metrics.csv exists otherwise delete it\n",
    "# utils.check_and_delete_metrics_file(config['history_folder'] + f\"client_{args.data_type}_{args.id}\", question=False)\n",
    "\n",
    "# load data\n",
    "data = np.load(f'./data/client_{1}.npy', allow_pickle=True).item()\n",
    "num_examples = data['train_features'].shape[0]\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset = models.CombinedDataset(data['train_features'], data['train_labels'], transform=None)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer and Loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=cfg.lr, momentum=cfg.momentum)\n",
    "\n",
    "\n",
    "for epoch in range(2):\n",
    "                models.simple_train(model, device, train_loader, optimizer, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def get_parameters(model, config):\n",
    "        return [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "\n",
    "def set_parameters(model, parameters):\n",
    "        params_dict = zip(model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "        model.load_state_dict(state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_parameters(model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_parameters(model, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CF_FL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
